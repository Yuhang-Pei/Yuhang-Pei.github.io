<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yuhang Pei</title>
    <link>https://yuhang-pei.github.io/</link>
    <description>Recent content on Yuhang Pei</description>
    <generator>Hugo -- 0.118.2</generator>
    <language>en</language>
    <atom:link href="https://yuhang-pei.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Test</title>
      <link>https://yuhang-pei.github.io/blog/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yuhang-pei.github.io/blog/test/</guid>
      <description>H1 The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.
A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space.</description>
    </item>
  </channel>
</rss>
